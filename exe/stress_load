#!/usr/bin/env ruby
require 'daemons'
require 'rufus-scheduler'
require 'mongo'
require 'parallel'
require_relative '../lib/helpers/user_record'
require_relative '../lib/mongo_cluster/replica_set'

class MongoClients

  attr_reader :queue

  def initialize(amount)
    @queue = Queue.new
    replica_members =
        MongoCluster::ReplicaSet
            .conf
            .fetch(:members)
            .map {|member| member.fetch(:host)}
    amount.times do
      client = Mongo::Client.new(replica_members, database: 'vatbox')['users']
      queue.push(client)
    end
  end

  def next
    queue
        .shift
        .tap {|client| queue.push(client)}
  end

end

client = MongoClients.new(1).next
ids = client.distinct(:id)
missing_ids = JSON.parse(File.read('/log/stress_load.log')).map {|user| user['id']}
lost_ids = Range.new(*ids.minmax).to_a - ids - missing_ids

Daemons.run_proc('stress_load', monitor: true, dir_mode: :system, log_output: true, log_dir: '/tmp') do
  scheduler = Rufus::Scheduler.new(max_work_threads: 200)
  threads = 150
  user_record = UserRecord.new
  mongo_clients = MongoClients.new(threads)
  mongo_clients.next.drop
  missing_ids = []
  log_file = Pathname('/log/stress_load.log')

  threads.times do

    scheduler.interval 1.second.to_i do
      begin
        user = user_record.next
        mongo_clients
            .next
            .insert_one(user)
      rescue => exception
        missing_ids.push(user)
        log_file.write(missing_ids.to_json)
      end
    end

  end

  scheduler.join
end